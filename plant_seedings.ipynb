{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, y, mini_batch_size):\n",
    "    m = X.shape[0]\n",
    "    mini_batches = []\n",
    "    \n",
    "    #Step 1: Shuffle\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = y[permutation]\n",
    "    \n",
    "    #Step 2: Partition\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    #Handling the end case\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[: m - mini_batch_size * num_complete_minibatches, :]\n",
    "        mini_batch_Y = shuffled_Y[: m - mini_batch_size * num_complete_minibatches]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_norm(s, axis=-1, epsilon=1e-7, keepdims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=keepdims)\n",
    "        return tf.sqrt(squared_norm + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis, keepdims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def primary_caps(conv1, caps1_n_maps, caps1_n_dims, caps1_n_caps, kernel_size=9, strides=2):\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=caps1_n_maps*caps1_n_dims, kernel_size=kernel_size, strides=strides, activation=tf.nn.relu)\n",
    "    caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims])\n",
    "    \n",
    "    return squash(caps1_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def digit_caps(caps1_output, W, caps2_n_caps, batch_size):\n",
    "    W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1])\n",
    "\n",
    "    caps1_output_expanded = tf.expand_dims(caps1_output, -1)\n",
    "    caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2)\n",
    "    caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1])\n",
    "    \n",
    "    caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled)\n",
    "\n",
    "    return caps2_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def routing(caps2_predicted, raw_weights, caps1_n_caps):\n",
    "    def condition(input, counter, caps2_predicted):\n",
    "        return tf.less(counter, 2)\n",
    "\n",
    "    def loop_body(raw_weights, counter, caps2_predicted):\n",
    "        # apply softmax function to compute the routing weights\n",
    "        routing_weights = tf.nn.softmax(raw_weights, axis=2)\n",
    "\n",
    "        # compute the weighted sum of all the predicted output vectors\n",
    "        weighted_predictions = tf.multiply(routing_weights, caps2_predicted)\n",
    "        weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True)\n",
    "\n",
    "        # apply the squash function to get outputs\n",
    "        caps2_output = squash(weighted_sum, axis=-2)\n",
    "\n",
    "        # tile the array and compute the scalar product\n",
    "        caps2_output_tiled = tf.tile(caps2_output, [1, caps1_n_caps, 1, 1, 1])\n",
    "        agreement = tf.matmul(caps2_predicted, caps2_output_tiled, transpose_a=True)\n",
    "\n",
    "        # update the routing weights by adding the scalar product\n",
    "        raw_weights = tf.add(raw_weights, agreement)\n",
    "\n",
    "        return raw_weights, tf.add(counter, 1), caps2_output_tiled\n",
    "\n",
    "    with tf.name_scope(\"routing\"):\n",
    "        counter = tf.constant(1)\n",
    "\n",
    "        raw_weights, counter, result = tf.while_loop(condition, loop_body, [raw_weights, counter, caps2_predicted])\n",
    "        caps2_output = tf.expand_dims(result[:, 1, :, :, :], 1)\n",
    "\n",
    "        return caps2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def caps_predicted_output(caps2_output):\n",
    "    # the length of the output vectors represent the class probabilites, use norm\n",
    "    y_proba = safe_norm(caps2_output, axis=-2)\n",
    "  \n",
    "    #select the one with the highest estimated probability to predict class\n",
    "    y_proba_argmax = tf.argmax(y_proba, axis=2)\n",
    "    y_pred = tf.squeeze(y_proba_argmax, axis=[1,2])\n",
    "  \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruction_input(caps2_output, mask_with_labels, y, y_pred, caps2_n_caps, caps2_n_dims):\n",
    "    # define the reconstruction targets\n",
    "    reconstruction_targets = tf.cond(mask_with_labels, lambda:y, lambda:y_pred)\n",
    "  \n",
    "    # create the reconstruction mask \n",
    "    reconstruction_mask = tf.one_hot(reconstruction_targets, depth=caps2_n_caps)\n",
    "    reconstruction_mask_reshaped = tf.reshape(reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1])\n",
    "  \n",
    "    #apply the mask\n",
    "    caps2_output_masked = tf.multiply(caps2_output, reconstruction_mask_reshaped)\n",
    "    decoder_input = tf.reshape(caps2_output_masked, [-1, caps2_n_caps * caps2_n_dims])\n",
    "  \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(caps2_output, decoder_output, T, X_flat, alpha=0.0005, m_plus=0.9, m_minus=0.1, lambda_=0.5):\n",
    "    # compute the norm of the output vector for each output capsule and each instance\n",
    "    caps2_output_norm = safe_norm(caps2_output, axis=-2, keepdims=True)\n",
    "\n",
    "    # compute present error\n",
    "    present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm))\n",
    "    present_error = tf.reshape(present_error_raw, shape=(-1,tf.shape(T)[1]))\n",
    "\n",
    "    # compute absent error\n",
    "    absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus))\n",
    "    absent_error = tf.reshape(absent_error_raw, shape=(-1,tf.shape(T)[1]))\n",
    "\n",
    "    # compute loss for each for each instance and each class\n",
    "    L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error)\n",
    "\n",
    "    # sum the digit losses and compute the mean over all instances\n",
    "    margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1))\n",
    "\n",
    "    # squared difference between the input image and the reconstructed image\n",
    "    squared_difference = tf.square(X_flat - decoder_output)\n",
    "    reconstruction_loss = tf.reduce_mean(squared_difference)\n",
    "\n",
    "    # sum of the margin loss and the reconstruction loss\n",
    "    # scaled down by alpha to ensure the margin loss dominates training\n",
    "    loss = tf.add(margin_loss, alpha * reconstruction_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(img_size, n_c):\n",
    "    # input X and y\n",
    "    X = tf.placeholder(shape=[None, img_size, img_size, n_c], dtype=tf.float32)\n",
    "    y = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "  \n",
    "    # placeholder for mask\n",
    "    mask_with_labels = tf.placeholder_with_default(False, shape=())\n",
    "  \n",
    "    return X, y, mask_with_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(parameters, init_sigma=0.1):\n",
    "    # init weights for digit caps\n",
    "    W_init = tf.random_normal(shape=(1, parameters['caps1_n_caps'], parameters['caps2_n_caps'], parameters['caps2_n_dims'], parameters['caps1_n_dims']),\n",
    "                         stddev=init_sigma, dtype=tf.float32)\n",
    "    W = tf.Variable(W_init)\n",
    "   \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, y, mask_with_labels, parameters, img_size):\n",
    "    # forward pass\n",
    "    conv1 = tf.layers.conv2d(X, filters=256, kernel_size=9, strides=1, activation=tf.nn.relu)\n",
    "    caps1_output = primary_caps(conv1, parameters['caps1_n_maps'], parameters['caps1_n_dims'], \n",
    "                                parameters['caps1_n_caps'], kernel_size=parameters['kernel_prime'], strides=2)\n",
    "    caps2_predicted = digit_caps(caps1_output, parameters['W'], parameters['caps2_n_caps'], tf.shape(X)[0])\n",
    "    #initialize new routing weights to zero\n",
    "    raw_weights = tf.zeros([tf.shape(X)[0], parameters['caps1_n_caps'], parameters['caps2_n_caps'], 1, 1], dtype=np.float32)\n",
    "    caps2_output = routing(caps2_predicted, raw_weights, parameters['caps1_n_caps'])\n",
    "  \n",
    "    # make prediction\n",
    "    y_pred = caps_predicted_output(caps2_output)\n",
    "  \n",
    "    # build decoder\n",
    "    decoder_input = reconstruction_input(caps2_output, mask_with_labels, y, y_pred, parameters['caps2_n_caps'], parameters['caps2_n_dims'])\n",
    "    #hidden1 = tf.layers.dense(decoder_input, parameters['n_hidden1'], activation=tf.nn.relu)\n",
    "    #hidden2 = tf.layers.dense(hidden1, parameters['n_hidden2'], activation=tf.nn.relu)\n",
    "    #decoder_output = tf.layers.dense(hidden2, img_size*img_size, activation=tf.nn.sigmoid)\n",
    "    fc1 = tf.contrib.layers.fully_connected(decoder_input, num_outputs=parameters['n_hidden1'])\n",
    "    fc1 = tf.reshape(fc1, shape=(tf.shape(X)[0], 5, 5, 16))\n",
    "    upsample1 = tf.image.resize_nearest_neighbor(fc1, (8, 8))\n",
    "    conv1 = tf.layers.conv2d(upsample1, 4, (3,3), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    upsample2 = tf.image.resize_nearest_neighbor(conv1, (16, 16))\n",
    "    conv2 = tf.layers.conv2d(upsample2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    upsample3 = tf.image.resize_nearest_neighbor(conv2, (32, 32))\n",
    "    conv6 = tf.layers.conv2d(upsample3, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    upsample4 = tf.image.resize_nearest_neighbor(conv6, (64, 64))\n",
    "    conv12 = tf.layers.conv2d(upsample4, 32, (3,3), padding='same', activation=tf.nn.relu)\n",
    "  \n",
    "    # 3 channel for RGG\n",
    "    logits = tf.layers.conv2d(conv12, 3, (3,3), padding='same', activation=None)\n",
    "    decoder_output = tf.nn.sigmoid(logits, name='decoded')\n",
    "    #tf.summary.image('reconstruction_img', decoded)\n",
    "  \n",
    "    return caps2_output, decoder_output, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_val, y_val, parameters, n_epochs = 10, batch_size = 50, img_size=(28,28,1), restore_checkpoint=True):\n",
    " \n",
    "    tf.reset_default_graph()                         \n",
    "    m = X_train.shape[0]\n",
    "    m_val = X_val.shape[0]\n",
    "    best_loss_val = np.infty\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "    \n",
    "    # Create Placeholders\n",
    "    X, y, mask_with_labels  = create_placeholders(img_size[0], img_size[2])\n",
    "\n",
    "    # Initialize parameters\n",
    "    W = initialize_parameters(parameters)\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    # Forward propagation\n",
    "    caps2_output, decoder_output, y_pred = forward_propagation(X, y, mask_with_labels, parameters, img_size[0])\n",
    "    \n",
    "    T = tf.one_hot(y, depth=caps2_n_caps)\n",
    "    X_flat = X#tf.reshape(X, [-1, img_size[0] * img_size[1] * img_size[2]])\n",
    "\n",
    "    # Cost function\n",
    "    loss = compute_loss(caps2_output, decoder_output, T, X_flat)\n",
    "    \n",
    "    # get accuracy\n",
    "    correct = tf.equal(y, y_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "            saver.restore(sess, checkpoint_path)\n",
    "        else:\n",
    "            init.run()\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            num_batches = int(m / batch_size)\n",
    "            minibatches = random_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "            for i, minibatch in enumerate(minibatches):\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # Run the session\n",
    "                _ , loss_train = sess.run([training_op, loss], feed_dict={X: minibatch_X,\n",
    "                                                                        y: minibatch_Y, mask_with_labels: True})\n",
    "                \n",
    "                print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                  i, num_batches, i * 100 / num_batches, loss_train),\n",
    "                end=\"\")\n",
    "\n",
    "            #at the end of each epoch, measure validation loss and accuracy\n",
    "            loss_vals = []\n",
    "            acc_vals = []\n",
    "            num_batches_val = int(m_val / batch_size)\n",
    "            minibatches_val = random_mini_batches(X_val, y_val, batch_size)\n",
    "            \n",
    "            for i, minibatch_val in enumerate(minibatches_val):\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch_val\n",
    "                                \n",
    "                # Run the session\n",
    "                loss_val , acc_val = sess.run([loss, accuracy], feed_dict={X: minibatch_X, \n",
    "                                                                           y: minibatch_Y})\n",
    "                loss_vals.append(loss_val)\n",
    "                acc_vals.append(acc_val)\n",
    "                \n",
    "                print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      i, num_batches_val,\n",
    "                      i * 100 / num_batches_val),\n",
    "                   end=\" \" * 10)\n",
    "                \n",
    "            loss_val = np.mean(loss_vals)\n",
    "            acc_val = np.mean(acc_vals)\n",
    "            print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "                    epoch + 1, acc_val * 100, loss_val,\n",
    "                    \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "            \n",
    "            # Save model if improved\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, checkpoint_path)\n",
    "                best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4275, 64, 64, 3)\n",
      "Epoch: 1  Val accuracy: 43.8000%  Loss: 0.398372 (improved)\n",
      "Epoch: 2  Val accuracy: 61.6000%  Loss: 0.313573 (improved)\n",
      "Epoch: 3  Val accuracy: 72.0000%  Loss: 0.256317 (improved)\n",
      "Epoch: 4  Val accuracy: 73.4000%  Loss: 0.239358 (improved)\n",
      "Epoch: 5  Val accuracy: 74.2000%  Loss: 0.237744 (improved)\n",
      "Epoch: 6  Val accuracy: 75.8000%  Loss: 0.233504 (improved)\n",
      "Epoch: 7  Val accuracy: 74.4000%  Loss: 0.238955\n",
      "Epoch: 8  Val accuracy: 74.4000%  Loss: 0.231604 (improved)\n",
      "Epoch: 9  Val accuracy: 75.0000%  Loss: 0.238329\n",
      "Epoch: 10  Val accuracy: 73.2000%  Loss: 0.249693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data = np.load('./out.npz')\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['arr_0'], data['arr_1'], test_size = 0.1)\n",
    "\n",
    "caps1_n_maps = 16\n",
    "caps1_n_caps = caps1_n_maps * 26 * 26\n",
    "caps1_n_dims = 16\n",
    "\n",
    "caps2_n_caps = 12\n",
    "caps2_n_dims = 32\n",
    "\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 1024\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "parameters =  { \"caps1_n_maps\": caps1_n_maps,\n",
    "                \"caps1_n_caps\": caps1_n_caps,\n",
    "                \"caps1_n_dims\": caps1_n_dims,\n",
    "                \"kernel_prime\": 5,\n",
    "                \"caps2_n_caps\": caps2_n_caps,\n",
    "                \"caps2_n_dims\": caps2_n_dims,\n",
    "                \"n_hidden1\": n_hidden1,\n",
    "                \"n_hidden2\": n_hidden2 }\n",
    "\n",
    "model(X_train, y_train, X_val, y_val, parameters, batch_size = 50, img_size=(64,64,3), restore_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
